This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    ci.yml
docs/
  _static/
    .gitignore
  authors.md
  changelog.md
  conf.py
  contributing.md
  index.md
  license.md
  Makefile
  readme.md
  requirements.txt
src/
  multinpainter/
    __init__.py
    __main__.py
    models.py
    multinpainter.py
    utils.py
tests/
  conftest.py
  test_skeleton.py
.coveragerc
.gitignore
.isort.cfg
.pre-commit-config.yaml
.readthedocs.yml
AUTHORS.md
CHANGELOG.md
LICENSE.txt
pyproject.toml
README.md
setup.cfg
setup.py
tox.ini
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/ci.yml">
# GitHub Actions configuration **EXAMPLE**,
# MODIFY IT ACCORDING TO YOUR NEEDS!
# Reference: https://docs.github.com/en/actions

name: tests

on:
  push:
    # Avoid using all the resources/limits available by checking only
    # relevant branches and tags. Other branches can be checked via PRs.
    branches: [main]
    tags: ['v[0-9]*', '[0-9]+.[0-9]+*']  # Match tags that resemble a version
  pull_request:  # Run in every PR
  workflow_dispatch:  # Allow manually triggering the workflow
  schedule:
    # Run roughly every 15 days at 00:00 UTC
    # (useful to check if updates on dependencies break the package)
    - cron: '0 0 1,16 * *'

concurrency:
  group: >-
    ${{ github.workflow }}-${{ github.ref_type }}-
    ${{ github.event.pull_request.number || github.sha }}
  cancel-in-progress: true

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      wheel-distribution: ${{ steps.wheel-distribution.outputs.path }}
    steps:
      - uses: actions/checkout@v3
        with: {fetch-depth: 0}  # deep clone for setuptools-scm
      - uses: actions/setup-python@v4
        id: setup-python
        with: {python-version: "3.11"}
      - name: Run static analysis and format checkers
        run: pipx run pre-commit run --all-files --show-diff-on-failure
      - name: Build package distribution files
        run: >-
          pipx run --python '${{ steps.setup-python.outputs.python-path }}'
          tox -e clean,build
      - name: Record the path of wheel distribution
        id: wheel-distribution
        run: echo "path=$(ls dist/*.whl)" >> $GITHUB_OUTPUT
      - name: Store the distribution files for use in other stages
        # `tests` and `publish` will use the same pre-built distributions,
        # so we make sure to release the exact same package that was tested
        uses: actions/upload-artifact@v3
        with:
          name: python-distribution-files
          path: dist/
          retention-days: 1

  test:
    needs: prepare
    strategy:
      matrix:
        python:
        - "3.7"  # oldest Python supported by PSF
        - "3.11"  # newest Python that is stable
        platform:
        - ubuntu-latest
        - macos-latest
        - windows-latest
    runs-on: ${{ matrix.platform }}
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        id: setup-python
        with:
          python-version: ${{ matrix.python }}
      - name: Retrieve pre-built distribution files
        uses: actions/download-artifact@v3
        with: {name: python-distribution-files, path: dist/}
      - name: Run tests
        run: >-
          pipx run --python '${{ steps.setup-python.outputs.python-path }}'
          tox --installpkg '${{ needs.prepare.outputs.wheel-distribution }}'
          -- -rFEx --durations 10 --color yes  # pytest args
      - name: Generate coverage report
        run: pipx run coverage lcov -o coverage.lcov
      - name: Upload partial coverage report
        uses: coverallsapp/github-action@master
        with:
          path-to-lcov: coverage.lcov
          github-token: ${{ secrets.github_token }}
          flag-name: ${{ matrix.platform }} - py${{ matrix.python }}
          parallel: true

  finalize:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Finalize coverage report
        uses: coverallsapp/github-action@master
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          parallel-finished: true

  publish:
    needs: finalize
    if: ${{ github.event_name == 'push' && contains(github.ref, 'refs/tags/') }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with: {python-version: "3.11"}
      - name: Retrieve pre-built distribution files
        uses: actions/download-artifact@v3
        with: {name: python-distribution-files, path: dist/}
      - name: Publish Package
        env:
          # TODO: Set your PYPI_TOKEN as a secret using GitHub UI
          # - https://pypi.org/help/#apitoken
          # - https://docs.github.com/en/actions/security-guides/encrypted-secrets
          TWINE_REPOSITORY: pypi
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}
        run: pipx run tox -e publish
</file>

<file path="docs/_static/.gitignore">
# Empty directory
</file>

<file path="docs/authors.md">
```{include} ../AUTHORS.md
:relative-docs: docs/
:relative-images:
```
</file>

<file path="docs/changelog.md">
```{include} ../CHANGELOG.md
:relative-docs: docs/
:relative-images:
```
</file>

<file path="docs/contributing.md">
```{include} ../CONTRIBUTING.md
:relative-docs: docs/
:relative-images:
```
</file>

<file path="docs/index.md">
# multinpainter

Iterative image outpainting powered by OpenAI Dall-E API


## Note

> This is the main page of your project's [Sphinx] documentation. It is
> formatted in [Markdown]. Add additional pages by creating md-files in
> `docs` or rst-files (formatted in [reStructuredText]) and adding links to
> them in the `Contents` section below.
>
> Please check [Sphinx] and [MyST] for more information
> about how to document your project and how to configure your preferences.


## Contents

```{toctree}
:maxdepth: 2

Overview <readme>
Contributions & Help <contributing>
License <license>
Authors <authors>
Changelog <changelog>
Module Reference <api/modules>
```

## Indices and tables

* {ref}`genindex`
* {ref}`modindex`
* {ref}`search`

[Sphinx]: http://www.sphinx-doc.org/
[Markdown]: https://daringfireball.net/projects/markdown/
[reStructuredText]: http://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html
[MyST]: https://myst-parser.readthedocs.io/en/latest/
</file>

<file path="docs/license.md">
# License

```{literalinclude} ../LICENSE.txt
:language: text
```
</file>

<file path="docs/Makefile">
# Makefile for Sphinx documentation
#

# You can set these variables from the command line, and also
# from the environment for the first two.
SPHINXOPTS    ?=
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = .
BUILDDIR      = _build
AUTODOCDIR    = api

# User-friendly check for sphinx-build
ifeq ($(shell which $(SPHINXBUILD) >/dev/null 2>&1; echo $?), 1)
$(error "The '$(SPHINXBUILD)' command was not found. Make sure you have Sphinx installed, then set the SPHINXBUILD environment variable to point to the full path of the '$(SPHINXBUILD)' executable. Alternatively you can add the directory with the executable to your PATH. If you don't have Sphinx installed, grab it from https://sphinx-doc.org/")
endif

.PHONY: help clean Makefile

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

clean:
	rm -rf $(BUILDDIR)/* $(AUTODOCDIR)

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
</file>

<file path="docs/readme.md">
```{include} ../README.md
:relative-docs: docs/
:relative-images:
```
</file>

<file path="docs/requirements.txt">
# Requirements file for ReadTheDocs, check .readthedocs.yml.
# To build the module reference correctly, make sure every external package
# under `install_requires` in `setup.cfg` is also listed here!
# sphinx_rtd_theme
myst-parser[linkify]
sphinx>=3.2.1
</file>

<file path="tests/test_skeleton.py">
import pytest

from multinpainter.skeleton import fib, main

__author__ = "twardoch"
__copyright__ = "twardoch"
__license__ = "Apache-2.0"


def test_fib():
    """API Tests"""
    assert fib(1) == 1
    assert fib(2) == 1
    assert fib(7) == 13
    with pytest.raises(AssertionError):
        fib(-10)


def test_main(capsys):
    """CLI Tests"""
    # capsys is a pytest fixture that allows asserts against stdout/stderr
    # https://docs.pytest.org/en/stable/capture.html
    main(["7"])
    captured = capsys.readouterr()
    assert "The 7-th Fibonacci number is 13" in captured.out
</file>

<file path=".coveragerc">
# .coveragerc to control coverage.py
[run]
branch = True
source = multinpainter
# omit = bad_file.py

[paths]
source =
    src/
    */site-packages/

[report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:
</file>

<file path=".isort.cfg">
[settings]
profile = black
known_first_party = multinpainter
</file>

<file path=".pre-commit-config.yaml">
exclude: '^docs/conf.py'

repos:
- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: v4.4.0
  hooks:
  - id: trailing-whitespace
  - id: check-added-large-files
  - id: check-ast
  - id: check-json
  - id: check-merge-conflict
  - id: check-xml
  - id: check-yaml
  - id: debug-statements
  - id: end-of-file-fixer
  - id: requirements-txt-fixer
  - id: mixed-line-ending
    args: ['--fix=auto']  # replace 'auto' with 'lf' to enforce Linux/Mac line endings or 'crlf' for Windows

## If you want to automatically "modernize" your Python code:
# - repo: https://github.com/asottile/pyupgrade
#   rev: v3.3.1
#   hooks:
#   - id: pyupgrade
#     args: ['--py37-plus']

## If you want to avoid flake8 errors due to unused vars or imports:
# - repo: https://github.com/PyCQA/autoflake
#   rev: v2.0.0
#   hooks:
#   - id: autoflake
#     args: [
#       --in-place,
#       --remove-all-unused-imports,
#       --remove-unused-variables,
#     ]

- repo: https://github.com/PyCQA/isort
  rev: 5.11.4
  hooks:
  - id: isort

- repo: https://github.com/psf/black
  rev: stable
  hooks:
  - id: black
    language_version: python3

## If like to embrace black styles even in the docs:
# - repo: https://github.com/asottile/blacken-docs
#   rev: v1.13.0
#   hooks:
#   - id: blacken-docs
#     additional_dependencies: [black]

- repo: https://github.com/PyCQA/flake8
  rev: 5.0.4
  hooks:
  - id: flake8
  ## You can add flake8 plugins via `additional_dependencies`:
  #  additional_dependencies: [flake8-bugbear]

## Check for misspells in documentation files:
# - repo: https://github.com/codespell-project/codespell
#   rev: v2.2.2
#   hooks:
#   - id: codespell
</file>

<file path=".readthedocs.yml">
# Read the Docs configuration file
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details

# Required
version: 2

# Build documentation in the docs/ directory with Sphinx
sphinx:
  configuration: docs/conf.py

# Build documentation with MkDocs
#mkdocs:
#  configuration: mkdocs.yml

# Optionally build your docs in additional formats such as PDF
formats:
  - pdf

build:
  os: ubuntu-22.04
  tools:
    python: "3.11"

python:
  install:
    - requirements: docs/requirements.txt
    - {path: ., method: pip}
</file>

<file path="AUTHORS.md">
# Contributors

- Adam Twardoch <adam+github@twardoch.com>
- ChatGPT
</file>

<file path="CHANGELOG.md">
# Changelog

## Version 0.1 (development)

- Feature A added
- FIX: nasty bug #1729 fixed
- add your changes here!
</file>

<file path="LICENSE.txt">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="pyproject.toml">
[build-system]
# AVOID CHANGING REQUIRES: IT WILL BE UPDATED BY PYSCAFFOLD!
requires = ["setuptools>=46.1.0", "setuptools_scm[toml]>=5"]
build-backend = "setuptools.build_meta"

[tool.setuptools_scm]
# For smarter version schemes and other configuration options,
# check out https://github.com/pypa/setuptools_scm
version_scheme = "no-guess-dev"
</file>

<file path="tox.ini">
# Tox configuration file
# Read more under https://tox.wiki/
# THIS SCRIPT IS SUPPOSED TO BE AN EXAMPLE. MODIFY IT ACCORDING TO YOUR NEEDS!

[tox]
minversion = 3.24
envlist = default
isolated_build = True


[testenv]
description = Invoke pytest to run automated tests
setenv =
    TOXINIDIR = {toxinidir}
passenv =
    HOME
    SETUPTOOLS_*
extras =
    testing
commands =
    pytest {posargs}


# # To run `tox -e lint` you need to make sure you have a
# # `.pre-commit-config.yaml` file. See https://pre-commit.com
# [testenv:lint]
# description = Perform static analysis and style checks
# skip_install = True
# deps = pre-commit
# passenv =
#     HOMEPATH
#     PROGRAMDATA
#     SETUPTOOLS_*
# commands =
#     pre-commit run --all-files {posargs:--show-diff-on-failure}


[testenv:{build,clean}]
description =
    build: Build the package in isolation according to PEP517, see https://github.com/pypa/build
    clean: Remove old distribution files and temporary build artifacts (./build and ./dist)
# https://setuptools.pypa.io/en/stable/build_meta.html#how-to-use-it
skip_install = True
changedir = {toxinidir}
deps =
    build: build[virtualenv]
passenv =
    SETUPTOOLS_*
commands =
    clean: python -c 'import shutil; [shutil.rmtree(p, True) for p in ("build", "dist", "docs/_build")]'
    clean: python -c 'import pathlib, shutil; [shutil.rmtree(p, True) for p in pathlib.Path("src").glob("*.egg-info")]'
    build: python -m build {posargs}
# By default, both `sdist` and `wheel` are built. If your sdist is too big or you don't want
# to make it available, consider running: `tox -e build -- --wheel`


[testenv:{docs,doctests,linkcheck}]
description =
    docs: Invoke sphinx-build to build the docs
    doctests: Invoke sphinx-build to run doctests
    linkcheck: Check for broken links in the documentation
passenv =
    SETUPTOOLS_*
setenv =
    DOCSDIR = {toxinidir}/docs
    BUILDDIR = {toxinidir}/docs/_build
    docs: BUILD = html
    doctests: BUILD = doctest
    linkcheck: BUILD = linkcheck
deps =
    -r {toxinidir}/docs/requirements.txt
    # ^  requirements.txt shared with Read The Docs
commands =
    sphinx-build --color -b {env:BUILD} -d "{env:BUILDDIR}/doctrees" "{env:DOCSDIR}" "{env:BUILDDIR}/{env:BUILD}" {posargs}


[testenv:publish]
description =
    Publish the package you have been developing to a package index server.
    By default, it uses testpypi. If you really want to publish your package
    to be publicly accessible in PyPI, use the `-- --repository pypi` option.
skip_install = True
changedir = {toxinidir}
passenv =
    # See: https://twine.readthedocs.io/en/latest/
    TWINE_USERNAME
    TWINE_PASSWORD
    TWINE_REPOSITORY
    TWINE_REPOSITORY_URL
deps = twine
commands =
    python -m twine check dist/*
    python -m twine upload {posargs:--repository {env:TWINE_REPOSITORY:testpypi}} dist/*
</file>

<file path="docs/conf.py">
# This file is execfile()d with the current directory set to its containing dir.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import os
import shutil
import sys

# -- Path setup --------------------------------------------------------------

__location__ = os.path.dirname(__file__)

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.join(__location__, "../src"))

# -- Run sphinx-apidoc -------------------------------------------------------
# This hack is necessary since RTD does not issue `sphinx-apidoc` before running
# `sphinx-build -b html . _build/html`. See Issue:
# https://github.com/readthedocs/readthedocs.org/issues/1139
# DON'T FORGET: Check the box "Install your project inside a virtualenv using
# setup.py install" in the RTD Advanced Settings.
# Additionally it helps us to avoid running apidoc manually

try:  # for Sphinx >= 1.7
    from sphinx.ext import apidoc
except ImportError:
    from sphinx import apidoc

output_dir = os.path.join(__location__, "api")
module_dir = os.path.join(__location__, "../src/multinpainter")
try:
    shutil.rmtree(output_dir)
except FileNotFoundError:
    pass

try:
    import sphinx

    cmd_line = f"sphinx-apidoc --implicit-namespaces -f -o {output_dir} {module_dir}"

    args = cmd_line.split(" ")
    if tuple(sphinx.__version__.split(".")) >= ("1", "7"):
        # This is a rudimentary parse_version to avoid external dependencies
        args = args[1:]

    apidoc.main(args)
except Exception as e:
    print(f"Running `sphinx-apidoc` failed!\n{e}")

# -- General configuration ---------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.intersphinx",
    "sphinx.ext.todo",
    "sphinx.ext.autosummary",
    "sphinx.ext.viewcode",
    "sphinx.ext.coverage",
    "sphinx.ext.doctest",
    "sphinx.ext.ifconfig",
    "sphinx.ext.mathjax",
    "sphinx.ext.napoleon",
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]


# Enable markdown
extensions.append("myst_parser")

# Configure MyST-Parser
myst_enable_extensions = [
    "amsmath",
    "colon_fence",
    "deflist",
    "dollarmath",
    "html_image",
    "linkify",
    "replacements",
    "smartquotes",
    "substitution",
    "tasklist",
]

# The suffix of source filenames.
source_suffix = [".rst", ".md"]

# The encoding of source files.
# source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = "index"

# General information about the project.
project = "multinpainter"
copyright = "2023, twardoch"

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# version: The short X.Y version.
# release: The full version, including alpha/beta/rc tags.
# If you don’t need the separation provided between version and release,
# just set them both to the same value.
try:
    from multinpainter import __version__ as version
except ImportError:
    version = ""

if not version or version.lower() == "unknown":
    version = os.getenv("READTHEDOCS_VERSION", "unknown")  # automatically set by RTD

release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
# language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
# Else, today_fmt is used as the format for a strftime call.
# today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ["_build", "Thumbs.db", ".DS_Store", ".venv"]

# The reST default role (used for this markup: `text`) to use for all documents.
# default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = "sphinx"

# A list of ignored prefixes for module index sorting.
# modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
# keep_warnings = False

# If this is True, todo emits a warning for each TODO entries. The default is False.
todo_emit_warnings = True


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = "alabaster"

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {"sidebar_width": "300px", "page_width": "1200px"}

# Add any paths that contain custom themes here, relative to this directory.
# html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
# html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
# html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
# html_logo = ""

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
# html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
# html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
# html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {}

# If false, no module index is generated.
# html_domain_indices = True

# If false, no index is generated.
# html_use_index = True

# If true, the index is split into individual pages for each letter.
# html_split_index = False

# If true, links to the reST sources are added to the pages.
# html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
# html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
# html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
# html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = "multinpainter-doc"


# -- Options for LaTeX output ------------------------------------------------

latex_elements = {
    # The paper size ("letterpaper" or "a4paper").
    # "papersize": "letterpaper",
    # The font size ("10pt", "11pt" or "12pt").
    # "pointsize": "10pt",
    # Additional stuff for the LaTeX preamble.
    # "preamble": "",
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
    ("index", "user_guide.tex", "multinpainter Documentation", "twardoch", "manual")
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = ""

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
# latex_use_parts = False

# If true, show page references after internal links.
# latex_show_pagerefs = False

# If true, show URL addresses after external links.
# latex_show_urls = False

# Documents to append as an appendix to all manuals.
# latex_appendices = []

# If false, no module index is generated.
# latex_domain_indices = True

# -- External mapping --------------------------------------------------------
python_version = ".".join(map(str, sys.version_info[0:2]))
intersphinx_mapping = {
    "sphinx": ("https://www.sphinx-doc.org/en/master", None),
    "python": ("https://docs.python.org/" + python_version, None),
    "matplotlib": ("https://matplotlib.org", None),
    "numpy": ("https://numpy.org/doc/stable", None),
    "sklearn": ("https://scikit-learn.org/stable", None),
    "pandas": ("https://pandas.pydata.org/pandas-docs/stable", None),
    "scipy": ("https://docs.scipy.org/doc/scipy/reference", None),
    "setuptools": ("https://setuptools.pypa.io/en/stable/", None),
    "pyscaffold": ("https://pyscaffold.org/en/stable", None),
}

print(f"loading configurations for {project} {version} ...", file=sys.stderr)
</file>

<file path="src/multinpainter/utils.py">
# Helper function to convert Image to png

import io

from PIL import Image


def image_to_png(image: Image.Image) -> bytes:
    png = io.BytesIO()
    image.save(png, format="PNG")
    return png.getvalue()
</file>

<file path="tests/conftest.py">
"""
Dummy conftest.py for multinpainter.

If you don't know what this is for, just leave it empty.
Read more about conftest.py under:
- https://docs.pytest.org/en/stable/fixture.html
- https://docs.pytest.org/en/stable/writing_plugins.html
"""

# import pytest
</file>

<file path=".gitignore">
# Temporary and binary files
*~
*.py[cod]
*.so
*.cfg
!.isort.cfg
!setup.cfg
*.orig
*.log
*.pot
__pycache__/*
.cache/*
.*.swp
*/.ipynb_checkpoints/*
.DS_Store

# Project files
.ropeproject
.project
.pydevproject
.settings
.idea
.vscode
tags

# Package files
*.egg
*.eggs/
.installed.cfg
*.egg-info

# Unittest and coverage
htmlcov/*
.coverage
.coverage.*
.tox
junit*.xml
coverage.xml
.pytest_cache/

# Build and docs folder/files
build/*
dist/*
sdist/*
docs/api/*
docs/_rst/*
docs/_build/*
cover/*
MANIFEST

# Per-project virtualenvs
.venv*/
.conda*/
.python-version
</file>

<file path="setup.py">
"""
Setup file for multinpainter.
Use setup.cfg to configure your project.

This file was generated with PyScaffold 4.4.
PyScaffold helps you to put up the scaffold of your new Python project.
Learn more under: https://pyscaffold.org/
"""

from setuptools import setup

if __name__ == "__main__":
    try:
        setup(use_scm_version={"version_scheme": "no-guess-dev"})
    except:  # noqa
        print(
            "\n\nAn error occurred while building the project, "
            "please ensure you have the most updated version of setuptools, "
            "setuptools_scm and wheel with:\n"
            "   pip install -U setuptools setuptools_scm wheel\n\n"
        )
        raise
</file>

<file path="src/multinpainter/models.py">
import base64
import io
import re
from typing import Any

import aiohttp
import httpx
import numpy as np
import openai
from PIL import Image

from .utils import image_to_png


# 1. describe_image
async def describe_image_hf(
    image: Image.Image,
    prompt_model: str,
    hf_api_key: str | None = None,
) -> str:
    headers = {"Authorization": f"Bearer {hf_api_key}"}
    api_url = f"https://api-inference.huggingface.co/models/{prompt_model}"
    png = image_to_png(image)

    async def post(
        api_url: str, image: bytes, headers: dict, wait_for_model: bool = True
    ) -> Any:
        async with aiohttp.ClientSession() as session:
            payload: dict[str, str] = {
                "inputs": {"image": base64.b64encode(image).decode("utf-8")},
                "options": {"wait_for_model": wait_for_model},
            }
            async with session.post(api_url, headers=headers, json=payload) as response:
                if not response.ok:
                    print(response)
                    print(headers)
                inference = await response.json()
        return inference

    inference = await post(api_url, png, headers=headers)
    pattern = re.compile(r"^Level \d+: ")
    description = inference[0].get("generated_text", "").strip()
    return re.sub(pattern, "", description)


# 2. detect_humans
def detect_humans_yolo(image: Image.Image) -> list[tuple[int, int, int, int]]:
    from ultralytics import YOLO

    boxes = []
    model = YOLO("yolov8n.pt")
    model.classes = [0]
    model.conf = 0.6
    detection = model.predict(image)

    for box_obj in detection:
        box = box_obj.boxes.xyxy.tolist()[0]
        boxes.append(
            (int(box[0] - 0.5), int(box[1] - 0.5), int(box[2] + 0.5), int(box[3] + 0.5))
        )

    return sorted(boxes, key=lambda box: box[0])


# 3. detect_faces
def detect_faces_dlib(image: Image.Image) -> Any | None:
    import dlib

    face_detector = dlib.get_frontal_face_detector()
    faces = face_detector(np.array(image.convert("RGB")), 1)
    return faces[0] if faces and len(faces) else None


# 4. inpaint_square
async def inpaint_square_openai(
    image: Image.Image,
    prompt: str,
    square_size: tuple[int, int],
    openai_api_key: str | None = None,
) -> Image.Image:
    openai.api_key = openai_api_key
    png = image_to_png(image)

    response = await openai.Image.acreate_edit(
        image=png,
        mask=png,
        prompt=prompt,
        n=1,
        size=f"{square_size[0]}x{square_size[1]}",
    )
    image_url = response["data"][0]["url"]

    async with httpx.AsyncClient() as client:
        response = await client.get(image_url)

    return Image.open(io.BytesIO(response.content))
</file>

<file path="src/multinpainter/__init__.py">
import sys

if sys.version_info[:2] >= (3, 8):
    # TODO: Import directly (no need for conditional) when `python_requires = >= 3.8`
    from importlib.metadata import PackageNotFoundError  # pragma: no cover
    from importlib.metadata import version
else:
    from importlib_metadata import PackageNotFoundError  # pragma: no cover
    from importlib_metadata import version

try:
    # Change here if project is renamed and does not equal the package name
    dist_name = __name__
    __version__ = version(dist_name)
except PackageNotFoundError:  # pragma: no cover
    __version__ = "unknown"
finally:
    del version, PackageNotFoundError

from .multinpainter import DESCRPTION_MODEL, Multinpainter_OpenAI

__all__ = ["Multinpainter_OpenAI", "DESCRPTION_MODEL"]
</file>

<file path="setup.cfg">
# This file is used to configure your project.
# Read more about the various options under:
# https://setuptools.pypa.io/en/latest/userguide/declarative_config.html
# https://setuptools.pypa.io/en/latest/references/keywords.html

[metadata]
name = multinpainter
description = Iterative image outpainting powered by OpenAI Dall-E API
author = Adam Twardoch
author_email = adam@twardoch.com
license = Apache-2.0
license_files = LICENSE.txt
long_description = file: README.md
long_description_content_type = text/markdown; charset=UTF-8; variant=GFM
url = https://github.com/twardoch/multinpainter-py
# Add here related links, for example:
project_urls =
    Documentation = https://github.com/twardoch/multinpainter-py
    Source = https://github.com/twardoch/multinpainter-py
    Tracker = https://github.com/twardoch/multinpainter-py/issues
#    Download = https://pypi.org/project/PyScaffold/#files

# Change if running only on Windows, Mac or Linux (comma-separated)
platforms = any

# Add here all kinds of additional classifiers as defined under
# https://pypi.org/classifiers/
classifiers =
    Development Status :: 4 - Beta
    Programming Language :: Python


[options]
zip_safe = False
packages = find_namespace:
include_package_data = True
package_dir =
    =src

# Require a min/specific Python version (comma-separated conditions)
# python_requires = >=3.8

# Add here dependencies of your project (line-separated), e.g. requests>=2.2,<3.0.
# Version specifiers like >=2.2,<3.0 avoid problems due to API changes in
# new major versions. This works if the required packages follow Semantic Versioning.
# For more information, check out https://semver.org/.
install_requires =
    importlib-metadata; python_version<"3.8"
    dlib>=19.24.0
    fire>=0.5.0
    numpy>=1.24.0
    openai>=0.27.0
    Pillow>=9.5.0
    requests>=2.28.0
    ultralytics
    tqdm
    httpx
    aiohttp

[options.packages.find]
where = src
exclude =
    tests

[options.extras_require]
# Add here additional requirements for extra features, to install with:
# `pip install multinpainter[PDF]` like:
# PDF = ReportLab; RXP

# Add here test requirements (semicolon/line-separated)
testing =
    setuptools
    pytest
    pytest-cov

[options.entry_points]
# Add here console scripts like:
console_scripts =
      multinpainter-py = multinpainter.__main__:cli
# For example:
# console_scripts =
#     fibonacci = multinpainter.skeleton:run
# And any other entry points, for example:
# pyscaffold.cli =
#     awesome = pyscaffoldext.awesome.extension:AwesomeExtension

[tool:pytest]
# Specify command line options as you would do when invoking pytest directly.
# e.g. --cov-report html (or xml) for html/xml output or --junitxml junit.xml
# in order to write a coverage file that can be read by Jenkins.
# CAUTION: --cov flags may prohibit setting breakpoints while debugging.
#          Comment those flags to avoid this pytest issue.
addopts =
    --cov multinpainter --cov-report term-missing
    --verbose
norecursedirs =
    dist
    build
    .tox
testpaths = tests
# Use pytest markers to select/deselect specific tests
# markers =
#     slow: mark tests as slow (deselect with '-m "not slow"')
#     system: mark end-to-end system tests

[devpi:upload]
# Options for the devpi: PyPI server and packaging tool
# VCS export must be deactivated since we are using setuptools-scm
no_vcs = 1
formats = bdist_wheel

[flake8]
# Some sane defaults for the code style checker flake8
max_line_length = 88
extend_ignore = E203, W503
# ^  Black-compatible
#    E203 and W503 have edge cases handled by black
exclude =
    .tox
    build
    dist
    .eggs
    docs/conf.py

[pyscaffold]
# PyScaffold's parameters when the project was created.
# This will be used when updating. Do not change!
version = 4.4
package = multinpainter
extensions =
    github_actions
    markdown
    no_tox
    pre_commit
</file>

<file path="src/multinpainter/__main__.py">
#!/usr/bin/env python3

import asyncio

import fire

from .multinpainter import Multinpainter_OpenAI


def get_inpainter(*args, **kwargs) -> Multinpainter_OpenAI:
    return Multinpainter_OpenAI(*args, **kwargs)


def inpaint(
    image: str,
    output: str,
    width: int,
    height: int,
    prompt: str | None = None,
    fallback: str | None = None,
    step: int | None = None,
    square: int = 1024,
    humans: bool = False,
    verbose: bool = False,
    openai_api_key: str | None = None,
    hf_api_key: str | None = None,
    prompt_model: str = None,
    *args,
    **kwargs,
) -> str:
    """
    Perform iterative inpainting on an image file using OpenAI's DALL-E 2 model.

    Args:
        image (str): Path to the input image file.
        output (str): Path to the output image file.
        width (int): Width of the output image in pixels.
        height (int): Height of the output image in pixels.
        prompt (str, optional): A prompt to guide the image generation.
        fallback (str, optional): A fallback prompt to use if no human is found in the image. If not provided but `humans` is specified, the tool will autogenerate the fallback prompt based on the main prompt.
        step (int, optional): The step size in pixels to move the window during the inpainting process. If not provided, the window will move by half the square size. Defaults to None.
        square (int, optional): The size of the square window to use for inpainting. Must be 1024 (default) or 512 or 256.
        humans (bool, optional): If specified, the algorithm will detect humans and apply the main prompt for squares with a human, and the fallback prompt for squares without a human.
        verbose (bool, optional): If specified, prints verbose info.
        openai_api_key (str, optional): Your OpenAI API key, defaults to the OPENAI_API_KEY environment variable.
        hf_api_key (str, optional): Your Huggingface API key, defaults to the HUGGINGFACEHUB_API_TOKEN env variable.
        prompt_model (str, optional): The Huggingface model to describe image. Defaults to "Salesforce/blip2-opt-2.7b".
    Returns:
        str: The path to the output image file.
    """

    inpainter = get_inpainter(
        image_path=image,
        out_path=output,
        out_width=width,
        out_height=height,
        prompt=prompt,
        fallback=fallback,
        step=step,
        square=square,
        humans=humans,
        verbose=verbose,
        openai_api_key=openai_api_key,
        hf_api_key=hf_api_key,
        prompt_model=prompt_model,
    )
    asyncio.run(inpainter.inpaint())
    return inpainter.out_path


def describe(
    image: str,
    hf_api_key: str | None = None,
    prompt_model: str = None,
    *args,
    **kwargs,
) -> str:
    """
    Describe the image.

    Args:
        image (str): Path to the input image file.
        hf_api_key (str, optional): Your Huggingface API key, defaults to the HUGGINGFACEHUB_API_TOKEN env variable.
        prompt_model (str, optional): The Huggingface model to describe image. Defaults to "Salesforce/blip2-opt-2.7b".
    Returns:
        str: The path to the output image file.
    """

    inpainter = get_inpainter(
        image_path=image,
        hf_api_key=hf_api_key,
        prompt_model=prompt_model,
    )
    asyncio.run(inpainter.describe_image())
    return inpainter.prompt


def cli():
    fire.core.Display = lambda lines, out: print(*lines, file=out)
    fire.Fire(
        {
            "inpaint": inpaint,
            "describe": describe,
        },
        name="multinpainter-py",
    )


if __name__ == "__main__":
    cli()
</file>

<file path="README.md">
# Multinpainter

Iterative image outpainting powered by OpenAI Dall-E API.

## Introduction

Multinpainter is a Python library and a CLI tool that can iteratively outpaint an input image using OpenAI’s API. 

You can specify an input image, the size of the output image, and optionally a prompt. Multinpainter will then iteratively call the OpenAI API to outpaint the image step-by-step until the entire output image is filled with content. 

You need an [OpenAI API key](https://platform.openai.com/account/api-keys). The tool performs a call to the GPT 3.5 API and then multiple calls to the Dall-E 2 API. 

If you don’t provide a prompt, Multinpainter will try to generate a prompt using the [`Salesforce/blip2-opt-2.7b`](https://huggingface.co/Salesforce/blip2-opt-2.7b) model on Huggingface. For that, you also need a [Huggingface access token](https://huggingface.co/settings/tokens). 

## Installation

```
python3 -m pip install --upgrade git+https://github.com/twardoch/multinpainter-py
```

## Usage

### Command-line

```
NAME
    multinpainter-py

SYNOPSIS
    multinpainter-py IMAGE OUTPUT WIDTH HEIGHT <flags>

DESCRIPTION
    Perform iterative inpainting on an image file using OpenAI's DALL-E 2 model.

POSITIONAL ARGUMENTS
    IMAGE
        Type: str
        Path to the input image file.
    OUTPUT
        Type: str
        Path to the output image file.
    WIDTH
        Type: int
        Width of the output image in pixels.
    HEIGHT
        Type: int
        Height of the output image in pixels.

FLAGS
    -p, --prompt=PROMPT
        Type: Optional[str]
        Default: None
        A prompt to guide the image generation.
    -f, --fallback=FALLBACK
        Type: Optional[str]
        Default: None
        A fallback prompt to use if no human is found in the image. 
        If not provided but `humans` is specified, the tool will 
        autogenerate the fallback prompt based on the main prompt.
    --step=STEP
        Type: Optional[int]
        Default: None
        The step size in pixels to move the window during the 
        inpainting process. If not provided, the window will 
        move by half the square size. Defaults to None.
    --square=SQUARE
        Type: int
        Default: 1024
        The size of the square window to use for inpainting. 
        Must be 1024 (default) or 512 or 256.
    -h, --humans=HUMANS
        Type: bool
        Default: False
        If specified, the algorithm will detect humans and apply 
        the main prompt for squares with a human, and the fallback 
        prompt for squares without a human.
    -v, --verbose=VERBOSE
        Type: bool
        Default: False
        If specified, prints verbose info.
    -a, --openai_api_key=API_KEY
        Type: Optional[str]
        Default: None
        Your OpenAI API key. If not provided, the API key will 
        be read from the OPENAI_API_KEY environment variable.
    --hf_api_key=HF_API_KEY
        Type: Optional[Str]
        Default: None
        The Huggingface API key. Defaults to None.
    --prompt_model=PROMPT_MODEL
        Type: Optional[str]
        Default: None
        The Huggingface model to describe image.

NOTES
    You can also use flags syntax for POSITIONAL ARGUMENTS
```

See below for explanation of the arguments. 

You can also use `python3 -m multinpainter` instead of `multinpainter-py`. 

### Python

In Python, you can also do: 

```python
import asyncio
from multinpainter import Multinpainter_OpenAI
inpainter = Multinpainter_OpenAI(
    image_path="input_image.png",
    out_path="output_image.png",
    out_width=1920,
    out_height=1080,
    prompt="Asian woman in front of blue wall",
    fallback="Solid blue wall",
    square=1024,
    step=256,
    humans=True,
    verbose=True,
    openai_api_key="sk-NNNNNN",
    hf_api_key="hf_NNNNNN",
    prompt_model="Salesforce/blip2-opt-2.7b"
)
asyncio.run(inpainter.inpaint())
```

When you initialize an instance of the `Multinpainter_OpenAI` class, it will: 

- Set up logging configurations.
- Open the input image and create an output image.
- Optionally detect humans in the input image using the YOLO model.
- Optionally detect faces in the input image using the Dlib library.
- Find the center of focus of the image (center of input image or the face if found).
- Calculate the expansion of the output image.
- Paste the input image onto the output image.
- Create the outpainting plan by generating a list of square regions in different directions.

You then call the `inpaint()` **async** method, which will:

- Optionally infer the prompt from the image.
- Perform outpainting for each square in the outpainting plan.
- Save the output image.
- Return the output image path.

### Arguments

Here’s an explanation of the arguments: 

| CLI                           | Python           | Explanation                                                                                                |
| ----------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------- |
| `IMAGE`                       | `image_path`     | The path of the input image to be outpainted.                                                              |
| `OUTPUT`                      | `out_path`       | The path where the output image will be saved.                                                             |
| `WIDTH`                       | `out_width`      | The desired width of the output image.                                                                     |
| `HEIGHT`                      | `out_height`     | The desired height of the output image.                                                                    |
| `-p PROMPT`                   | `prompt`         | The main prompt that will guide the outpainting process.                                                   |
| `-f FALLBACK`                 | `fallback`       | A fallback prompt used for outpainting when no humans are detected in the image.                           |
| `--square=SQUARE`             | `square`         | The size of the square region that will be outpainted during each step , must be `1024` or `512` or `256`. |
| `--step=STEP`                 | `step`           | The step size used to move the outpainting square, half of square by default.                              |
| `--humans`                    | `humans`         | A boolean flag indicating whether to detect humans in the image and adapt the prompt accordingly.          |
| `--verbose`                   | `verbose`        | If given, prints verbose output and saves intermediate outpainting images.                                 |
| `-a API_KEY`                  | `openai_api_key` | OpenAI API key or OPENAI_API_KEY env variable.                                                             |
| `--hf_api_key=HF_API_KEY`     | `hf_api_key`     | Huggingface API key or `HUGGINGFACEHUB_API_TOKEN` env variable.                                            |
| `--prompt_model=PROMPT_MODEL` | `prompt_model`   | The Huggingface model to describe image. Defaults to `Salesforce/blip2-opt-2.7b`.                          |

See docstrings inside the code for more detailed info.

## Authors & License

- Copyright (c) 2023 Adam Twardoch, licensed under [Apache 2.0](./LICENSE.txt).
- Written with significant assistance from ChatGPT-4.

## Note

This project has been set up using PyScaffold 4.4. For details and usage
information on PyScaffold see https://pyscaffold.org/.
</file>

<file path="src/multinpainter/multinpainter.py">
import json
import logging
import os
from collections import OrderedDict
from datetime import datetime
from pathlib import Path

import openai
from PIL import Image
from tqdm import tqdm

__author__ = "Adam Twardoch"
__license__ = "Apache-2.0"

DESCRPTION_MODEL = "Salesforce/blip2-opt-2.7b"


class Multinpainter_OpenAI:
    f"""
    A class for iterative inpainting using OpenAI's Dall-E 2 and GPT-3 atificial intelligence models to extend (outpaint) an existing image to new defined dimensions.

    Args:
        image_path (str): Path to the input image file.
        out_path (str): Path to save the output image file.
        out_width (int): Desired width of the output image.
        out_height (int): Desired height of the output image.
        prompt (str, optional): Prompt for the GPT-3 model to generate image content.
        fallback (str, optional): Fallback prompt to use when the original prompt contains
            human-related items. Defaults to None.
        step (int, optional): The number of pixels to shift the square in each direction
            during the iterative inpainting process. Defaults to None.
        square (int, optional): Size of the square region to inpaint in pixels. Defaults to 1024.
        humans (bool, optional): Whether to include human-related items in the prompt.
            Defaults to True.
        verbose (bool, optional): Whether to enable verbose logging. Defaults to False.
        openai_api_key (str, optional): OpenAI API key or OPENAI_API_KEY env variable.
        hf_api_key (str, optional): Huggingface API key or HUGGINGFACEHUB_API_TOKEN env variable.
        prompt_model (str, optional): The Huggingface model to describe image. Defaults to "{DESCRPTION_MODEL}".

    A class for iterative inpainting using OpenAI's Dall-E 2 and GPT-3 models to generate image content from an input image and prompt.

    Attributes:
        image_path (str): Path to input image file.
        out_path (str): Path to save output image file.
        out_width (int): Desired width of output image.
        out_height (int): Desired height of output image.
        prompt (str): Prompt for GPT-3 model to generate image content.
        fallback (str): Fallback prompt to use when the original prompt contains human-related items. Defaults to None.
        step (int): The number of pixels to shift the square in each direction during the iterative inpainting process. Defaults to None.
        square (int): Size of the square region to inpaint in pixels. Defaults to 1024.
        humans (bool): Whether to include human-related items in the prompt. Defaults to True.
        verbose (bool): Whether to enable verbose logging. Defaults to False.
        openai_api_key (str): OpenAI API key or OPENAI_API_KEY env variable.
        hf_api_key (str): Huggingface API key or HUGGINGFACEHUB_API_TOKEN env variable.
        prompt_model (str): Huggingface model to describe image. Defaults to "{DESCRPTION_MODEL}".
        input_width (int): Width of input image.
        input_height (int): Height of input image.
        image (PIL.Image.Image): Input image as a PIL.Image object.
        out_image (PIL.Image.Image): Output image as a PIL.Image object.
        center_of_focus (Tuple[int,int]): Coordinates of center of focus in input image.
        expansion (Tuple[int,int,int,int]): Expansion values for input image to fit output size.
        human_boxes (List[Tuple[int,int,int,int]]): List of bounding boxes for detected humans in input image.

    Methods:
        configure_logging(): Sets up logging configuration based on verbose flag.
        timestamp(): Returns current timestamp in format '%Y%m%d-%H%M%S'.
        snapshot(): Takes snapshot of current output image and saves to disk.
        open_image(): Opens input image and converts to RGBA format.
        save_image(): Saves output image to disk in PNG format.
        to_rgba(image): Converts input image to RGBA format and returns.
        to_png(image): Converts input image to PNG format and returns binary data.
        make_prompt_fallback(): Generates fallback prompt if given prompt contains human-related items.
        create_out_image(): Creates new RGBA image of size out_width x out_height with transparent background and returns.
        describe_image(): Generates prompt using a Huggingface image captioning model.
        detect_humans(): Detects humans in input image using YOLOv5 model from ultralytics package.
        detect_faces(): Detects a face in input image using dlib face detector.
        find_center_of_focus(): Finds center of focus for output image.
        calculate_expansion(): Calculates amount of expansion needed to fit input image into output image while maintaining center of focus.
        paste_input_image(): Pastes input image onto output image, taking into account calculated expansion.
        openai_inpaint(png, prompt): Calls OpenAI's Image Inpainting API to inpaint given square of output image.
        get_initial_square_position(): Calculates position of initial square to be inpainted.
        human_in_square(square_box): Checks if given square contains human.
        inpaint_square(square_delta): Inpaints given square based on square_delta.
        create_planned_squares(): Generates list of squares to be inpainted in a specific order.
        move_square(square_delta, direction): Moves given square in specified direction by step size.
        iterative_inpainting(): Performs iterative inpainting process by calling inpaint_square() method on each square in planned square list.
        inpaint(): Asynchronous main entry point for Multinpainter_OpenAI class.

    Usage:
        import asyncio
        from multinpainter import Multinpainter_OpenAI
        inpainter = Multinpainter_OpenAI(
            image_path="input_image.png",
            out_path="output_image.png",
            out_width=1920,
            out_height=1080,
            prompt="Asian woman in front of blue wall",
            fallback="Solid blue wall",
            square=1024,
            step=256,
            humans=True,
            verbose=True,
            openai_api_key="sk-NNNNNN",
            hf_api_key="hf_NNNNNN",
            prompt_model="{DESCRPTION_MODEL}"
        )
        asyncio.run(inpainter.inpaint())
    """

    def __init__(
        self,
        image_path: str | Path,
        out_path: str | Path = None,
        out_width: int = 0,
        out_height: int = 0,
        prompt: str | None = None,
        fallback: str | None = None,
        step: int | None = None,
        square: int = 1024,
        humans: bool = True,
        verbose: bool = False,
        openai_api_key: str | None = None,
        hf_api_key: str | None = None,
        prompt_model: str = None,
    ):
        f"""
        - Initialize the Multinpainter_OpenAI instance with the required input parameters.
        - Set up logging configurations.
        - Open the input image and create an output image.
        - Optionally detect humans in the input image using the YOLO model.
        - Optionally detect faces in the input image using the Dlib library.
        - Find the center of focus of the image (center of input image or the face if found).
        - Calculate the expansion of the output image.
        - Paste the input image onto the output image.
        - Create the outpainting plan by generating a list of square regions in different directions.

        Args:
            image_path (Union[str, Path]): The path of the input image file.
            out_path (Union[str, Path]): The path for the output inpainted image file.
            out_width (int): The width of the output image.
            out_height (int): The height of the output image.
            prompt (str, optional): The prompt text to be used in the inpainting process.
            fallback (str, optional): The fallback prompt text, used when inpainting non-human areas. Defaults to None.
            step (int, optional): The step size to move the inpainting square. Defaults to None.
            square (int, optional): The size of the inpainting square. Defaults to 1024.
            humans (bool, optional): Whether to consider humans in the inpainting process. Defaults to True.
            verbose (bool, optional): Whether to show verbose logging. Defaults to False.
            openai_api_key (str, optional): Your OpenAI API key, defaults to the OPENAI_API_KEY environment variable.
            hf_api_key (str, optional): Your Huggingface API key, defaults to the HUGGINGFACEHUB_API_TOKEN env variable.
            prompt_model (str, optional): The Huggingface model to describe image. Defaults to "{DESCRPTION_MODEL}".
        """
        self.verbose = verbose
        self.configure_logging()
        logging.info("Starting iterative OpenAI inpainter...")
        self.openai_api_key = openai_api_key or os.environ.get("OPENAI_API_KEY", None)
        openai.openai_api_key = self.openai_api_key
        self.hf_api_key = hf_api_key or os.environ.get("HUGGINGFACEHUB_API_TOKEN", None)
        self.image_path = Path(image_path)
        logging.info(f"Image path: {self.image_path}")
        self.open_image()
        self.out_width = out_width
        self.out_height = out_height
        if not out_path:
            out_path = self.image_path.with_name(
                f"{self.image_path.stem}_outpainted-{self.out_width}x{self.out_height}.png"
            )
        self.out_path = Path(out_path)
        logging.info(f"Output path: {self.out_path}")
        logging.info(f"Output size: {self.out_width}x{self.out_height}")
        self.prompt = prompt
        self.fallback = fallback
        self.prompt_model = (
            prompt_model or DESCRPTION_MODEL
        )  # "Salesforce/blip2-opt-6.7b-coco" #
        self.square = square
        self.step = step or square // 2
        self.center_of_focus = None
        self.humans = humans
        self.face_boxes = None

    def prep_inpainting(self):
        logging.info(f"Square size: {self.square}")
        logging.info(f"Step size: {self.step}")
        self.out_image = self.create_out_image()
        self.detect_faces()
        self.find_center_of_focus()
        self.expansion = self.calculate_expansion()
        self.human_boxes = self.detect_humans() if self.humans else []
        if len(self.human_boxes):
            self.make_prompt_fallback()
        self.paste_input_image()
        self.planned_squares = self.create_planned_squares()

    def configure_logging(self) -> None:
        """
        Configures the logging settings for the application.
        """
        log_level = logging.DEBUG if self.verbose else logging.WARNING
        logging.basicConfig(level=log_level, format="%(levelname)s: %(message)s")

    def timestamp(self) -> str:
        """
        Returns the current timestamp in the format 'YYYYMMDD-HHMMSS'.

        Returns:
            str: The current timestamp as a string.
        """
        return datetime.now().strftime("%Y%m%d-%H%M%S")

    def snapshot(self) -> None:
        """
        Saves a snapshot of the current output image with a timestamp in the file name. Only saves the snapshot if the verbose flag is set to True.
        """
        if self.verbose:
            snapshot_path = Path(
                self.out_path.parent,
                f"{self.out_path.stem}-{self.timestamp()}.png",
            )
            logging.info(f"Saving snapshot: {snapshot_path}")
            self.out_image.save(
                snapshot_path,
                format="PNG",
            )

    def open_image(self) -> None:
        """
        Opens the input image from the specified image path, converts it to RGBA format, and stores the image and its dimensions as instance variables.
        """
        self.image = self.to_rgba(Image.open(self.image_path))
        self.input_width, self.input_height = self.image.size
        logging.info(f"Input size: {self.input_width}x{self.input_height}")

    def save_image(self) -> None:
        """
        Saves the output image to the specified output path with a PNG format.
        """
        self.out_image.save(self.out_path.with_suffix(".png"), format="PNG")
        logging.info(f"Output image saved to: {self.out_path}")

    def to_rgba(self, image: Image) -> Image:
        """
        Converts the given image to RGBA format and returns the converted image.

        Args:
            image (Image): The input image to be converted.

        Returns:
            Image: The converted RGBA image.
        """

        return image.convert("RGBA")

    def make_prompt_fallback(self):
        """
        Generates a non-human version of the prompt using the GPT-3.5-turbo model.
        The method updates the instance variable `prompt_fallback` with the non-human version of the prompt.
        If a fallback prompt is already provided, this method does nothing.
        """

        if self.fallback:
            return False
        prompt = f"""Create a JSON dictionary. Rewrite this text into one Python list of short phrases, focusing on style, on the background, and on overall scenery, but ignoring humans and human-related items: "{self.prompt_human}". Put that list in the `descriptors` item. In the `ignored` item, put a list of the items from the `descriptors` list that have any relation to humans, human activity or human properties. In the `approved` item, put a list of the items from the `descriptors` list which are not in the `ignore` list, but also include items from the `descriptors` list that relate to style or time. Output only the JSON dictionary, no commentary or explanations."""
        logging.info(f"Adapting to non-human prompt:\n{prompt}")
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": ""},
                {"role": "user", "content": prompt},
            ],
        )
        result = response.choices[0].message.content
        logging.info(f"Non-human prompt result: {result}")
        try:
            prompt_fallback = json.loads(result).get("approved", [])
            self.prompt_fallback = ", ".join(prompt_fallback) + ", no humans"
            logging.info(f"Non-human prompt: {self.prompt_fallback}")
        except json.decoder.JSONDecodeError:
            logging.warning(f"Invalid non-human prompt: {result}")

    def create_out_image(self):
        """
        Creates the output image by combining the input image and the generated text.
        The method uses the instance variables `image` and `generated_text` to create the final output image.
        The resulting image is stored in the instance variable `out_image`.
        """
        return Image.new("RGBA", (self.out_width, self.out_height), (0, 0, 0, 0))

    async def describe_image(self, func_describe=None, *args, **kwargs):
        if func_describe is None:
            from .models import describe_image_hf

            func_describe = describe_image_hf

        logging.info("Describing image...")
        self.prompt = await func_describe(
            self.image, self.prompt_model, self.hf_api_key, *args, **kwargs
        )

    def detect_humans(self, func_detect=None, *args, **kwargs):
        """
        Detects human faces or bodies in the input image using a pre-trained model.
        The method processes the instance variable `image` and returns a list of detected human bounding boxes.
        Each bounding box is represented as a tuple (x, y, width, height).

        Returns:
            list: A list of detected human bounding boxes in the input image.
        """
        if func_detect is None:
            from .models import detect_humans_yolov8

            func_detect = detect_humans_yolov8

        self.human_boxes = func_detect(self.image, *args, **kwargs)
        logging.info(f"Detected humans: {self.human_boxes}")

    def detect_faces(self, func_detect=None, *args, **kwargs):
        """
        Detects human faces in the input image using a pre-trained model.
        The method processes the instance variable `image` and returns a list of detected face bounding boxes.
        Each bounding box is represented as a tuple (x, y, width, height).

        Returns:
            list: A list of detected face bounding boxes in the input image.
        """
        if func_detect is None:
            from .models import detect_faces_dlib

            func_detect = detect_faces_dlib

        self.face_boxes = func_detect(self.image, *args, **kwargs)
        logging.info(f"Detected faces: {self.face_boxes}")

    def find_center_of_focus(self):
        """
        Calculates the center of focus in the input image based on the positions of detected faces and humans.
        The method processes the instance variable `image` and uses the results of `detect_faces` and `detect_humans`
        to determine an optimal focal point for cropping or other image processing tasks.

        Returns:
            tuple: A tuple (x, y) representing the coordinates of the calculated center of focus in the input image.
        """
        if self.face_boxes:
            x_min, y_min, x_max, y_max = self.face_boxes[0]
            center_x = (x_min + x_max) // 2
            center_y = (y_min + y_max) // 2
            self.center_of_focus = center_x, center_y
        else:
            self.center_of_focus = self.image.size[0] // 2, self.image.size[1] // 2
        logging.info(f"Center of focus: {self.center_of_focus}")

    def calculate_expansion(self):
        x_percentage = self.center_of_focus[0] / self.input_width
        y_percentage = self.center_of_focus[1] / self.input_height

        x_left = int((self.out_width - self.input_width) * x_percentage)
        x_right = self.out_width - self.input_width - x_left
        y_top = int((self.out_height - self.input_height) * y_percentage)
        y_bottom = self.out_height - self.input_height - y_top

        return x_left, x_right, y_top, y_bottom

    def paste_input_image(self):
        """
        Pastes the input image onto the output image, considering the calculated expansion values.
        This method ensures that the input image is placed onto the output image, taking into account
        the expansion values to position the input image correctly within the output image canvas.
        """
        self.out_image.paste(self.image, (self.expansion[0], self.expansion[2]))

    def get_initial_square_position(self):
        """
        Calculates the initial position of the square used for inpainting.

        Returns:
            Tuple[int, int]: The initial (x, y) position of the top-left corner of the square.
        """
        x_init = max(0, self.expansion[0] - (self.square - self.input_width) // 2)
        y_init = max(0, self.expansion[2] - (self.square - self.input_height) // 2)
        return x_init, y_init

    def human_in_square(self, square_box: tuple[int, int, int, int]) -> bool:
        """
        Determines whether any detected human bounding boxes intersect with the given square_box.

        Args:
            square_box (Tuple[int, int, int, int]): The (x0, y0, x1, y1) coordinates of the square_box.

        Returns:
            bool: True if any detected human bounding boxes intersect with the square_box, False otherwise.
        """
        x0, y0, x1, y1 = square_box

        for box in self.human_boxes:
            bx0, by0, bx1, by1 = box
            if x0 < bx1 and x1 > bx0 and y0 < by1 and y1 > by0:
                return True
        return False

    async def inpaint_square(
        self, square_delta: tuple[int, int], func_inpaint=None, *args, **kwargs
    ) -> None:
        """
        Inpaints the square region in the output image specified by square_delta using OpenAI's API.
        Chooses the appropriate prompt based on the presence of humans in the square.

        Args:
            square_delta (Tuple[int, int]): The (x, y) coordinates of the top-left corner of the square region.

        Returns:
            None
        """
        if func_inpaint is None:
            from .models import inpaint_square_openai

            func_inpaint = inpaint_square_openai

        x, y = square_delta
        x1, y1 = x + self.square, y + self.square
        if (
            x >= self.expansion[0]
            and y >= self.expansion[2]
            and x1 <= self.expansion[0] + self.input_width
            and y1 <= self.expansion[2] + self.input_height
        ):
            return

        square = self.out_image.crop((x, y, x1, y1))

        if self.human_in_square((x, y, x1, y1)):
            prompt = self.prompt_human
        else:
            prompt = self.prompt_fallback

        logging.info(f"Inpainting region {x} {y} {x1} {y1} with: {prompt}")
        inpainted_square = await func_inpaint(
            square,
            prompt,
            (self.square, self.square),
            self.openai_api_key,
            *args,
            **kwargs,
        )
        self.out_image.paste(inpainted_square, (x, y))
        self.snapshot()

    def create_planned_squares(self):
        """
        Generates a dictionary that represents the order in which the image squares will be processed during the inpainting process.
        The dictionary has the following keys:
        - `init`: contains the initial square position.
        - `up`: contains the squares above the initial square, in the order they should be processed.
        - `left`: contains the squares to the left of the initial square, in the order they should be processed.
        - `right`: contains the squares to the right of the initial square, in the order they should be processed.
        - `down`: contains the squares below the initial square, in the order they should be processed.
        - `up_left`: contains the squares above and to the left of the initial square, in the order they should be processed.
        - `up_right`: contains the squares above and to the right of the initial square, in the order they should be processed.
        - `down_left`: contains the squares below and to the left of the initial square, in the order they should be processed.
        - `down_right`: contains the squares below and to the right of the initial square, in the order they should be processed.

        Each key in the dictionary is associated with a list of square positions that represent the order in which the inpainting process will occur.
        The order is determined by starting from the initial square and iterating over each direction (up, down, left, right) until there is no more space in that direction.
        Then, for each combination of up/down and left/right directions, the squares are ordered diagonally.

        Returns:
        The generated dictionary.
        """

        init_square = self.get_initial_square_position()

        planned_squares = OrderedDict(
            init=[init_square],
            up=[],
            left=[],
            right=[],
            down=[],
            up_left=[],
            up_right=[],
            down_left=[],
            down_right=[],
        )

        # Calculate up, left, right, and down squares
        x, y = init_square
        for direction in ["up", "left", "right", "down"]:
            cur_x, cur_y = x, y
            while True:
                cur_x, cur_y = self.move_square((cur_x, cur_y), direction)
                if cur_x is None or cur_y is None:
                    break
                planned_squares[direction].append((cur_x, cur_y))

        # Calculate up_left, up_right, down_left, and down_right squares
        for up_down in ["up", "down"]:
            for left_right in ["left", "right"]:
                quadrant = f"{up_down}_{left_right}"
                for up_sq in planned_squares[up_down]:
                    for lr_sq in planned_squares[left_right]:
                        quadrant_sq = (lr_sq[0], up_sq[1])
                        planned_squares[quadrant].append(quadrant_sq)

        logging.info(f"Planned squares: {planned_squares}")
        return planned_squares

    def move_square(
        self, square_delta: tuple[int, int], direction: str
    ) -> tuple[int, int]:
        """
        Calculates the position of the square in a given direction.

        Args:
        - square_delta: A tuple (x, y) representing the position of the square.
        - direction: A string representing the direction of the movement. Can be one of 'up', 'down', 'left', 'right'.

        Returns:
        - A tuple (x, y) representing the new position of the square after the movement in the given direction.
        - If the new position is outside the image, the corresponding coordinate is set to None.
        """

        x, y = square_delta

        if direction == "up":
            next_y = max(0, y - self.step)
            if next_y == y:
                return x, None
            return x, next_y
        elif direction == "left":
            next_x = max(0, x - self.step)
            if next_x == x:
                return None, y
            return next_x, y
        elif direction == "right":
            next_x = min(x + self.step, self.out_width - self.square)
            if next_x == x:
                return None, y
            return next_x, y
        elif direction == "down":
            next_y = min(y + self.step, self.out_height - self.square)
            if next_y == y:
                return x, None
            return x, next_y

    async def iterative_inpainting(self):
        """
        Iteratively performs the inpainting process by calling `inpaint_square` on each square in the order defined by `create_planned_squares`.
        Initializes and updates a progress bar to track the progress of the inpainting process.
        """
        if not self.prompt:
            self.prompt = await self.describe_image()
        self.prompt_human = self.prompt
        logging.info(f"Homan prompt: {self.prompt_human}")
        self.prompt_fallback = self.fallback or self.prompt
        logging.info(f"Fallback prompt: {self.prompt_fallback}")

        inpainting_plan = [
            square_delta
            for direction in self.planned_squares
            for square_delta in self.planned_squares[direction]
        ]
        progress_bar = tqdm(
            inpainting_plan, desc="Outpainting square", total=len(inpainting_plan)
        )
        for square_delta in progress_bar:
            await self.inpaint_square(square_delta)

    async def inpaint(self):
        """
        - Asynchronously perform outpainting for each square in the outpainting plan.
        - Save the output image.
        """
        self.prep_inpainting()
        await self.iterative_inpainting()
        self.save_image()
</file>

</files>
